from docx import Document
from .gpt import GPTClient
import re
import fitz
import pandas as pd
from .config import PASSAGE_IMAGE_DIR
from .logger import LogLevel, get_logger

logger = get_logger()

class Parser:
    def __init__(self, ori_language: str = "en", trans_language: str = "zh", api_key: str = None, session_dir: str = None):
        # å»¶é²åˆå§‹åŒ– GPTClientï¼Œåªæœ‰åœ¨éœ€è¦æ™‚æ‰å‰µå»º
        self._api_key = api_key
        self._session_dir = session_dir
        self._gpt_client = None
        # å¦‚æœæä¾›äº† session_dirï¼Œå°‡åœ–ç‰‡å„²å­˜åœ¨ session_dir ä¸­ï¼Œå¦å‰‡ä½¿ç”¨é è¨­ç›®éŒ„
        if session_dir:
            from pathlib import Path
            self.passage_image_path = str(Path(session_dir))
        else:
            self.passage_image_path = PASSAGE_IMAGE_DIR
        self.ori_language = ori_language
        self.trans_language = trans_language
    
    @property
    def gpt_client(self):
        """å»¶é²åˆå§‹åŒ– GPTClient"""
        if self._gpt_client is None:
            self._gpt_client = GPTClient(api_key=self._api_key, session_dir=self._session_dir)
        return self._gpt_client
    
    def execute(self, *args, **kwargs):

        if kwargs.get("parser_type") == "passage":
            self.parse_pdf(kwargs.get("pdf_path"))
            vocab_list = self.gpt_client.passage_with_question(self.passage_image_path)
            logger.log(LogLevel.DEBUG, f"vocab_list: {vocab_list}")
            return vocab_list

        elif kwargs.get("parser_type") == "word":
            ...

        elif kwargs.get("parser_type") == "excel":
            ...
        

    def parse_word(self, path: str):
        doc = Document(path)

        current_word = None
        word_data = {}
        all_words = []

        for para in doc.paragraphs:
            text = para.text.strip()
            if not text:
                continue

            logger.log(LogLevel.DEBUG, f"\n =====> åŸå§‹æ®µè½: {text}")
            logger.log(LogLevel.DISPLAY, "-"*80)

            # 1ï¸âƒ£ æª¢æŸ¥æ˜¯å¦ç‚ºæ–°å–®å­—æ®µè½
            found_word = self._find_docx_word(text)
            if found_word:
                # å„²å­˜å‰ä¸€å€‹å–®å­—è³‡æ–™
                if current_word and word_data:
                    all_words.append(word_data)

                current_word = found_word
                word_data = {
                    "word": current_word,
                    "chinese": "",
                    "pos": "",
                    "examples": []
                }
                logger.log(LogLevel.INFO, f"æ‰¾åˆ°æ–°å–®å­—: {current_word}")


                chinese = self._find_docx_chinese(text)

                if chinese and word_data:
                    word_data["chinese"] = chinese
                    logger.log(LogLevel.INFO, f"ä¸­æ–‡è§£é‡‹: {chinese}")


                pos = self._find_docx_pos(text)
                if pos and word_data:
                    word_data["pos"] = pos
                    logger.log(LogLevel.INFO, f"è©æ€§: {pos}")


            # 4ï¸âƒ£ Bullet ä¾‹å¥ï¼ˆä¸­è‹±å°ç…§ï¼‰
            en, zh = self._find_docx_bulleted_list(para)
            if en and zh and word_data:
                word_data["examples"].append({"en": en, "zh": zh})
                logger.log(LogLevel.INFO, f"ğŸ“Bulletä¾‹å¥: EN='{en}' / ZH='{zh}'")
                continue

        # æœ€å¾Œä¸€å€‹å–®å­—åŠ é€²ä¾†
        if current_word and word_data:
            all_words.append(word_data)

        logger.log(LogLevel.SUCCESS, f"\n ç¸½å…±è§£æ {len(all_words)} å€‹å–®å­—")
        return all_words
    
    def _find_docx_bulleted_list(self, para):
        """åˆ¤æ–·æ˜¯å¦ç‚º bullet list ä¸¦æ‹†ä¸­è‹±"""
        is_bullet = bool(para._element.xpath(".//w:numPr"))
        if is_bullet:
            lines = [line.strip() for line in para.text.splitlines() if line.strip()]
            if len(lines) == 2:
                en, zh = lines
                return en, zh
        return None, None

    def _find_docx_pos(self, text: str):
        pos_match = re.search(r'è©æ€§ï¼š(.+)', text)
        return pos_match.group(1).strip() if pos_match else None

    def _find_docx_chinese(self, text: str):
        chinese_match = re.search(r'ä¸­æ–‡ï¼š(.+)', text)
        return chinese_match.group(1).strip() if chinese_match else None

    def _find_docx_word(self, text: str):
        word_match = re.match(r'^(\d+)\.\s+(\w+)', text)
        return word_match.group(2) if word_match else None

    def parse_pdf_text(self, path: str):
        """è§£æ pdf çš„æ–‡å­—"""
        ...

    def parse_pdf(self, path: str):
        # å…ˆå˜—è©¦æ–‡å­—ï¼›è‹¥ç„¡æ–‡å­—å†æŠ½åœ–
        text_found = False
        try:
            import pdfplumber
            with pdfplumber.open(path) as pdf:
                for p in pdf.pages:
                    if p.extract_text():
                        text_found = True
                        break
        except Exception:
            text_found = False

        if text_found:
            # TODO: ä¹‹å¾Œå¦‚éœ€ç›´æ¥ä»¥æ–‡å­—é€² LLMï¼Œå¯åœ¨æ­¤å¯¦ä½œ
            return
        self.parse_pdf_image(path)

    def parse_pdf_image(self, pdf_path: str):
        """è§£æ pdf çš„åœ–ç‰‡"""
        import os
        from pathlib import Path
        
        os.makedirs(self.passage_image_path, exist_ok=True)

        # å–å¾— PDF æª”æ¡ˆåç¨±ï¼ˆä¸å«å‰¯æª”åï¼‰
        pdf_name = Path(pdf_path).stem
        
        doc = fitz.open(pdf_path)
        image_counter = 1  # åœ–ç‰‡è¨ˆæ•¸å™¨ï¼Œå¾ p1 é–‹å§‹
        
        for page_index in range(len(doc)):
            page = doc[page_index]
            images = page.get_images(full=True)
            logger.log(LogLevel.INFO, f"ç¬¬ {page_index + 1} é åŒ…å« {len(images)} å¼µåœ–ç‰‡")

            for image_index, img in enumerate(images):
                xref = img[0]  # å–å‡ºåœ–åƒçš„ XREF ID
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                image_ext = base_image["ext"]

                # å„²å­˜åœ–ç‰‡ï¼šPDF åç¨± + p1, p2, p3...
                from helpers.file_utils import slugify
                safe_pdf_name = slugify(pdf_name)
                image_filename = f"{safe_pdf_name}_p{image_counter}.{image_ext}"
                out_path = os.path.join(self.passage_image_path, image_filename)
                with open(out_path, "wb") as f:
                    f.write(image_bytes)

                logger.log(LogLevel.SUCCESS, f"å„²å­˜åœ–ç‰‡ï¼š{image_filename}")
                image_counter += 1  # éå¢è¨ˆæ•¸å™¨

    def parse_excel(self, path: str):
        if not path:
            raise ValueError("è«‹æä¾› Excel æª”æ¡ˆè·¯å¾‘ã€‚")

        try:
            if path.lower().endswith(".csv"):
                df = pd.read_csv(path)
            else:
                df = pd.read_excel(path)
        except Exception as e:
            raise ValueError(f"ç„¡æ³•è®€å– Excelï¼š{e}") from e

        if df.empty:
            return []

        alias_map = {
            "word": ("word", "å–®å­—", "vocab", "è©å½™"),
            "pos": ("pos", "è©æ€§"),
            "meaning": ("meaning", "æ„æ€", "ä¸­æ–‡æ„æ€"),
            "synonyms": ("synonyms", "åŒç¾©è©"),
            "ex1_ori": ("ex1_ori", "ä¾‹å¥1", "ä¾‹å¥1è‹±æ–‡", "ä¾‹å¥ä¸€è‹±æ–‡"),
            "ex1_trans": ("ex1_trans", "ä¾‹å¥1ç¿»è­¯", "ä¾‹å¥1ä¸­æ–‡", "ä¾‹å¥ä¸€ä¸­æ–‡"),
            "ex2_ori": ("ex2_ori", "ä¾‹å¥2", "ä¾‹å¥2è‹±æ–‡", "ä¾‹å¥äºŒè‹±æ–‡"),
            "ex2_trans": ("ex2_trans", "ä¾‹å¥2ç¿»è­¯", "ä¾‹å¥2ä¸­æ–‡", "ä¾‹å¥äºŒä¸­æ–‡"),
            "audio": ("audio", "èªéŸ³", "éŸ³æª”"),
        }

        def normalize(name: str) -> str:
            return str(name).strip().lower()

        normalized_columns = {normalize(col): col for col in df.columns}

        column_mapping = {}
        for field, aliases in alias_map.items():
            for alias in aliases:
                normalized_alias = normalize(alias)
                if normalized_alias in normalized_columns:
                    column_mapping[field] = normalized_columns[normalized_alias]
                    break

        if "word" not in column_mapping:
            raise ValueError("Excel æª”æ¡ˆç¼ºå°‘å¿…éœ€æ¬„ä½ï¼šwordï¼ˆæˆ–å°æ‡‰åŒç¾©åç¨±ï¼‰ã€‚")

        vocab_fields = [
            "word",
            "pos",
            "meaning",
            "synonyms",
            "ex1_ori",
            "ex1_trans",
            "ex2_ori",
            "ex2_trans",
            "audio",
        ]

        vocab_list = []
        for _, row in df.iterrows():
            word = row.get(column_mapping["word"], "")
            if pd.isna(word) or not str(word).strip():
                continue

            entry = {}
            for field in vocab_fields:
                source_col = column_mapping.get(field)
                value = row.get(source_col, "") if source_col else ""
                entry[field] = "" if pd.isna(value) else str(value).strip()
            vocab_list.append(entry)

        return vocab_list

    def parse_vocab_txt(self, vocab_path: str):
        """
        è®€å–è©å½™æ¸…å–®æª”æ¡ˆï¼Œä¸¦å›å‚³è©å½™æ¸…å–®
        """
        with open(vocab_path, "r", encoding="utf-8") as f:
            text = f.read()

        vocab_list = [word for word in text.splitlines()]
        return vocab_list

    def to_excel(self, data: list[dict]):
        ...


if __name__ == "__main__":
    parser = Parser()
    parser.execute(parser_type="passage", pdf_path="./sample/IELTS 19 Test 1 Reading Passage 1 é¡Œç›®-1.pdf")
